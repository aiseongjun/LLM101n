{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2a24ec4f83e54b",
   "metadata": {},
   "source": [
    "# Lecture 2: Introduction to Language Modeling\n",
    "\n",
    "In this lecture, we will introduce the concept of language modeling and implement a bigram language model.\n",
    "\n",
    "Let's make a bigram language model that generates names character by character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ae07f393cdc97",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "Bigram language model is a statistical language model that predicts the next token based on the current token.\n",
    "\n",
    "For example, given the sentence \"A cat sat on the mat\"\n",
    "- A -> cat\n",
    "- cat -> sat\n",
    "- sat -> on\n",
    "- on -> the\n",
    "- the -> mat\n",
    "\n",
    "For practical reasons, let's use a character-level language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1304792293fd5",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7815b2d251de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from utils import load_text, set_seed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2d969efe31622",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b312f599e32ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = BigramConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade6330195eb42b",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b00f2922d17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config.seed)\n",
    "generator = torch.Generator().manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094b1515c1b76bf",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bdfd4710591cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959aa3381411c95",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8271b7ba9a5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Print some info about the dataset                                            #\n",
    "# number of names, the first 5 names, min/max name length, etc.                #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(names[:5])\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e1b292fbdc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot name length\n",
    "plt.hist([len(name) for name in names], bins=20)\n",
    "plt.xlabel(\"Name Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Name Length Histogram\")\n",
    "plt.show()\n",
    "\n",
    "# Plot alphabet frequency\n",
    "alphabet = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "alphabet_counts = torch.zeros(len(alphabet))\n",
    "for name in names:\n",
    "    for char in name:\n",
    "        alphabet_counts[ord(char) - 97] += 1\n",
    "plt.bar(alphabet, alphabet_counts)\n",
    "plt.xlabel(\"Alphabet\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Alphabet Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897812a14f6c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot what alphabet comes after other alphabets\n",
    "alphabet2idx = {char: idx for idx, char in enumerate(alphabet)}\n",
    "idx2alphabet = {idx: char for char, idx in alphabet2idx.items()}\n",
    "alphabet_matrix = torch.zeros(len(alphabet), len(alphabet), dtype=torch.int32)\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        alphabet_matrix[alphabet2idx[char1], alphabet2idx[char2]] += 1\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(alphabet_matrix, cmap='Blues')\n",
    "for i in range(len(alphabet)):\n",
    "    for j in range(len(alphabet)):\n",
    "        chstr = idx2alphabet[i] + idx2alphabet[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, alphabet_matrix[i, j].item(), ha='center', va='top', color='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# This is the statistics we will use to build the bigram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c449cf8150292d",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b1b1b853b51d9",
   "metadata": {},
   "source": [
    "#### 1. Add special tokens\n",
    "\n",
    "The matrix above has a problem. It does not indicate the beginning and end of a name.\n",
    "- The model does not know what alphabet to start with.\n",
    "- The model does not know when to stop generating a name.\n",
    "\n",
    "To indicate the beginning and end of a name, we need to add special tokens.\n",
    "- Beginning: `<S>`\n",
    "- End: `<E>`\n",
    "\n",
    "For example, the name \"emma\" will be \"`<S>`emma`<E>`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118fa163da6543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special token\n",
    "names = [\"<S>\" + name + \"<E>\" for name in names]\n",
    "print(f\"First 5 names: {names[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1f4cbaeda638b",
   "metadata": {},
   "source": [
    "Let's use the **same** special token for the beginning and end of a name.\n",
    "\n",
    "Why? Those two tokens will not get mixed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a3a86cedba8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200d8db3eb0cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Add the special token '.' to the beginning and end of each name.             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "names = ['.' + name + '.' for name in names]\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"First 5 names: {names[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e857cb7d617c",
   "metadata": {},
   "source": [
    "#### 2. Tokenizer\n",
    "\n",
    "To feed the names to the model, we need to convert them to integers. Tokenization is the process of converting text to integers.\n",
    "\n",
    "We will use a character-level tokenizer. ('.' -> 0, 'a' -> 1, 'b' -> 2 , ..., 'z' -> 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233d57d9c077193",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "print(f\"Characters: {chars}\")\n",
    "print(f\"Vocab size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9189405cd56339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}\n",
    "print(f\"str2idx: {str2idx}\")\n",
    "print(f\"idx2str: {idx2str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd074059faa5273",
   "metadata": {},
   "source": [
    "### Part 1: Statistical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b670189d4118b2d",
   "metadata": {},
   "source": [
    "#### Statistics\n",
    "\n",
    "What alphabet is likely to come after another alphabet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d492b2c974d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor to store what alphabet comes after other alphabets\n",
    "char_matrix = torch.zeros(config.vocab_size, config.vocab_size, dtype=torch.int32)\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        char_matrix[str2idx[char1], str2idx[char2]] += 1\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(char_matrix, cmap='Blues')\n",
    "for i in range(config.vocab_size):\n",
    "    for j in range(config.vocab_size):\n",
    "        stat = idx2str[i] + idx2str[j]\n",
    "        plt.text(j, i, stat, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, char_matrix[i, j].item(), ha='center', va='top', color='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93944ad52232b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Get the probabilities by dividing the counts by the total counts             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "prob = char_matrix / char_matrix.sum(dim=-1).view(-1, 1)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94801fbb3f688a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the probabilities for the first alphabet\n",
    "plt.bar([idx2str[i] for i in range(config.vocab_size)], prob[0, :])\n",
    "plt.xlabel(\"Alphabet\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"First Alphabet Probability\")\n",
    "plt.show()\n",
    "print(prob[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c86c11a1732dc",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bee88cfac09f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(probs):\n",
    "    new_name = []\n",
    "    start_idx = str2idx[\".\"]\n",
    "    \n",
    "    while True:\n",
    "        # Sample\n",
    "        print(\"------------------\")\n",
    "        print(f\"Input: {idx2str[start_idx]}\")\n",
    "        print(f\"Probabilities: {probs[start_idx]}\")\n",
    "        next_idx = torch.multinomial(probs[start_idx], num_samples=1, generator=generator).item()\n",
    "        \n",
    "        # Decode\n",
    "        new_char = idx2str[next_idx]\n",
    "        print(f\"Output (probability): {new_char} ({probs[start_idx, next_idx]:.4f})\")\n",
    "        new_name.append(new_char)\n",
    "        \n",
    "        # Update\n",
    "        start_idx = next_idx\n",
    "        \n",
    "        if next_idx == str2idx[\".\"]:\n",
    "            break\n",
    "            \n",
    "    return ''.join(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4d77e32684eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random name\n",
    "uniform_prob = torch.ones(config.vocab_size, config.vocab_size) / config.vocab_size\n",
    "for _ in range(1):\n",
    "    print(generate_name(uniform_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb005d983a9b3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a name based on the probabilities\n",
    "for _ in range(1):\n",
    "    print(generate_name(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874b04595d68b35",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "How should we evaluate the quality of the generated names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e4454fa8ba956",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# What is the probability of generating a random new token?                    #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "random_prob = 1 / 27\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"1 / vocab size: {random_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc814480ced6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names[:3]:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        char1_id = str2idx[char1]\n",
    "        char2_id = str2idx[char2]\n",
    "        print(f\"'{char1}' -> '{char2}': P={prob[char1_id, char2_id]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e90e54bf220ad4",
   "metadata": {},
   "source": [
    "Intuitively, we want to maximize the probability of generating the correct next token given the current token.\n",
    "- Not learned: 1 / vocab_size = 0.037\n",
    "- If learned, higher the better (should be higher than 0.037)\n",
    "\n",
    "How can we summarize into a single number, that shows the quality of the model?\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)**: Product of all probabilities\n",
    "- Why use logarithm? multiplication -> addition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b263b6a31bfe46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names[:3]:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        char1_id = str2idx[char1]\n",
    "        char2_id = str2idx[char2]\n",
    "        print(f\"'{char1}' -> '{char2}': P={prob[char1_id, char2_id]:.4f}. LogP={torch.log(prob[char1_id, char2_id]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bac7968a4542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log likelihood of first 3 names\n",
    "# log(a*b) = log(a) + log(b)\n",
    "log_likelihood = 0\n",
    "n = 0\n",
    "for name in names[:3]:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        char1_id = str2idx[char1]\n",
    "        char2_id = str2idx[char2]\n",
    "        n += 1\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Log likelihood of the correct next token given the current token             #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        log_likelihood += torch.log(prob[char1_id, char2_id])\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"Average Log likelihood: {-log_likelihood / n:.4f}\")\n",
    "# We designed the loss function. yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82420ee25705216",
   "metadata": {},
   "source": [
    "**Summary of what we've done so far**\n",
    "\n",
    "Bigram Language Model\n",
    "- Parameters: Tensor of size (vocab_size, vocab_size) -> Statistical approach\n",
    "- Evaluation: Average Log likelihood of the correct next token given the current token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30369932b7cbbe1",
   "metadata": {},
   "source": [
    "### Part 2: Neural Network approach\n",
    "\n",
    "This time, we will use gradient descent and backpropagation to learn the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689567e5031f227",
   "metadata": {},
   "source": [
    "#### Dataloader\n",
    "\n",
    "We need to create a dataset of (Input, Target) pairs.\n",
    "- Input: Current token\n",
    "- Target: Next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b76604dac8dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Input, Target pairs\n",
    "example_name = names[0] # emma.\n",
    "print(f\"Name: {example_name}\")\n",
    "\n",
    "example_input0 = str2idx[\".\"]  # 0\n",
    "example_target0 = str2idx[\"e\"]  # 5\n",
    "print(f\"Input: {example_input0}, Target: {example_target0}\")\n",
    "\n",
    "example_input1 = str2idx[\"e\"]  # 5\n",
    "example_target1 = str2idx[\"m\"]  # 13\n",
    "print(f\"Input: {example_input1}, Target: {example_target1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62a7bae177d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of Input, Target pairs\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Append the input string and target string to the inputs and targets list.    #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        inputs.append(str2idx[char1])   \n",
    "        targets.append(str2idx[char2])\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# Convert to tensor\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"First (Input, Target): ({inputs[0]}, {targets[0]})\")\n",
    "print(f\"Second (Input, Target): ({inputs[1]}, {targets[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d85bfde838c3c",
   "metadata": {},
   "source": [
    "#### One-hot Encoding\n",
    "\n",
    "The model cannot take integers as input. (e.g., Input: 5, Target: 13)\n",
    "\n",
    "The model expects a vector. One way to convert integers to vectors is to use one-hot encoding.\n",
    "- Input: 5 -> [0, 0, 0, 0, 0, 1, 0, ..., 0]\n",
    "- Target: 1 -> [0, 1, 0, 0, ..., 0]\n",
    "\n",
    "PyTorch provides One-Hot Encoding functionality. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)\n",
    "\n",
    "Read the documentation and implement the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0b03c28eca5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Implement one-hot encoding for inputs and targets.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "inputs_encoded = torch.nn.functional.one_hot(inputs, num_classes=config.vocab_size)\n",
    "targets_encoded = torch.nn.functional.one_hot(targets, num_classes=config.vocab_size)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Convert data type to float\n",
    "inputs_encoded = inputs_encoded.float()\n",
    "targets_encoded = targets_encoded.float()\n",
    "\n",
    "print(f\"Input shape: {inputs_encoded.shape}\")\n",
    "print(f\"Target shape: {targets_encoded.shape}\")\n",
    "print(f\"Input dtype: {inputs_encoded.dtype}\")\n",
    "print(f\"Target dtype: {targets_encoded.dtype}\")\n",
    "print(f\"First Input: {inputs_encoded[0]}\")\n",
    "print(f\"First Target: {targets_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666528f98f7c9a7",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Let's define the weights for a simple linear model.\n",
    "\n",
    "y = Wx + b\n",
    "- x: (data_size, vocab_size)\n",
    "- W: (vocab_size, vocab_size)\n",
    "- b: (vocab_size)\n",
    "- y: (data_size, vocab_size)\n",
    "\n",
    "What is torch.randn? [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.randn.html)\n",
    "\n",
    "Why add requires_grad=True?\n",
    "- If the tensor has requires_grad=False, PyTorch will not track operations on it, and it will not be able to calculate gradients. (saving memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb4664cc8c14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W = torch.randn(config.vocab_size, config.vocab_size, generator=generator, requires_grad=True)\n",
    "b = torch.randn(config.vocab_size, generator=generator, requires_grad=True)\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"b shape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1746bd541253f4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76fd109ef84eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of forward pass\n",
    "# data_size = 1\n",
    "logits = torch.matmul(inputs_encoded[0], W) + b  # or use @\n",
    "print(f\"Input: {inputs_encoded[0]}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Logits: {logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f914d76c956a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969442826e2cff9",
   "metadata": {},
   "source": [
    "Q: What do we do to get the probability of an output?\n",
    "\n",
    "A: **Softmax**\n",
    "- Softmax converts logits to probabilities.\n",
    "- Softmax: exp(x) / sum(exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48973a87e7ba228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability matrix\n",
    "prob = logits.exp() / logits.exp().sum(dim=-1, keepdim=True)\n",
    "print(f\"Probability shape: {prob.shape}\")\n",
    "print(f\"Probability: {prob}\")\n",
    "print(f\"Sum of Probability: {prob.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604f1340e38f7b8",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35bff990f7e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "next_idx = torch.multinomial(prob, num_samples=1, generator=generator).item()\n",
    "print(f\"Input: {inputs_encoded[0]}\")\n",
    "print(f\"Output (probability): {next_idx} ({prob[next_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ec0f5276563f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
